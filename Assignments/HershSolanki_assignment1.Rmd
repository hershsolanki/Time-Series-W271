---
title : 'W271 Assignment 1'
subtitle: 'Due 11:59pm Pacific Time, Sunday February 2, 2020'
output: 
  pdf_document:
  toc: true
  number_sections: true
fontsize: 11pt
geometry: margin=1in
---
**Hersh Solanki**

## Instructions (Please Read Carefully):

* **Late submissions will not be accepted**

* No page limit, but be reasonable

* Do not modify fontsize, margin or line_spacing settings

* This assignment needs to be completed individually; this is not a group project

* Submission is by pushing to your student fork of the course repository

* Submit two files:
    
    1. A pdf file that details your answers (knit to pdf, do not knit to html then save as pdf). Include all R code used to produce the answers. Do not suppress the code in your pdf file
    
    2. The R markdown (Rmd) file used to produce the pdf file
  
    The assignment will not be graded unless **both** files are submitted
      
* Use the following file-naming convensation:
    * StudentFirstNameLastName_HWNumber.fileExtension
    * For example, if the student's name is Kyle Cartman for assignment 1, name your files follows:
        * KyleCartman_assignment1.Rmd
        * KyleCartman_assignment1.pdf
            
* Although it sounds obvious, please write your name on page 1 of your pdf and Rmd files

* Answers should clearly explain your reasoning; do not simply 'output dump' the results of code without explanation 

* For statistical methods that we cover in this course, use the R libraries and functions that are covered in this course. If you use libraries and functions for statistical modeling that we have not covered, you must provide an explanation of why such libraries and functions are used and reference the library documentation. For data wrangling and data visualization, you are free to use other libraries, such as dplyr, ggplot2, etc.

* For mathematical formulae, type them in your R markdown file. Do not e.g. write them on a piece of paper, snap a photo, and use the image file.

* Incorrectly following submission instructions results in deduction of grades

* Students are expected to act with regard to UC Berkeley Academic Integrity

\newpage
# 1. Confidence Intervals (2 points)

A Wald confidence interval for a binary response probability does not always have the stated confidence level, $1-\alpha$, where $\alpha$ (the probability of rejecting the null hypothesis when it is true) is often set to $0.05\%$. This was demonstrated with code in the week 1 live session file.

**Question 1.1:** Use the code from the week 1 live session file and: (1) redo the exercise for `n=50, n=100, n=500`, (2) plot the graphs, and (3) describe what you have observed from the results. Use the same `pi.seq` as in the live session code.

```{r, message=FALSE}
one.one = function(n) {
  
pi = 0.6
alpha = 0.05
n = n
w = 0:n

wald.CI.true.coverage = function(pi, alpha=0.05, n) {
    
    w = 0:n
  
    pi.hat = w/n
    pmf = dbinom(x=w, size=n, prob=pi)
    
    var.wald = pi.hat*(1-pi.hat)/n
    wald.CI_lower.bound = pi.hat - qnorm(p = 1-alpha/2)*sqrt(var.wald)
    wald.CI_upper.bound = pi.hat + qnorm(p = 1-alpha/2)*sqrt(var.wald)
    
    covered.pi = ifelse(test = pi>wald.CI_lower.bound, yes = ifelse(test = pi<wald.CI_upper.bound, yes=1, no=0), no=0)
    
    wald.CI.true.coverage = sum(covered.pi*pmf)
    
    wald.df = data.frame(w, pi.hat, round(data.frame(pmf, wald.CI_lower.bound,wald.CI_upper.bound),4), covered.pi)
    
    return(wald.df)
  }
  
  wald.df = wald.CI.true.coverage(pi=0.6, alpha=0.05, n=n)
  wald.CI.true.coverage.level = sum(wald.df$covered.pi*wald.df$pmf)
  
  # Let's compute the ture coverage for a sequence of pi
  pi.seq = seq(0.01,0.99, by=0.01)
  wald.CI.true.matrix = matrix(data=NA,nrow=length(pi.seq),ncol=2)
  counter=1
  for (pi in pi.seq) {
      wald.df2 = wald.CI.true.coverage(pi=pi, alpha=0.05, n=n)
      #print(paste('True Coverage is', sum(wald.df2$covered.pi*wald.df2$pmf)))
      wald.CI.true.matrix[counter,] = c(pi,sum(wald.df2$covered.pi*wald.df2$pmf))
      counter = counter+1
  }
  str(wald.CI.true.matrix)
  wald.CI.true.matrix[1:5,]
  
  # Plot the true coverage level (for given n and alpha)
  plot(x=wald.CI.true.matrix[,1],
       y=wald.CI.true.matrix[,2],
       ylim=c(0,1),
       main = "Wald C.I. True Confidence Level Coverage", xlab=expression(pi),
       ylab="True Confidence Level",
       type="l")
  abline(h=1-alpha, lty="dotted")
}

one.one(50)
one.one(100)
one.one(500)
```
 
**Question 1.2:** (1) Modify the code for the Wilson Interval. (2) Do the exercise for `n=10, n=50, n=100, n=500`. (3) Plot the graphs. (4) Describe what you have observed from the results and compare the Wald and Wilson intervals based on your results. Use the same `pi.seq` as in the live session code.

```{r, message=FALSE}
two.two = function(n) {
  
pi = 0.6
alpha = 0.05
n = n
w = 0:n

wilson.CI.true.coverage = function(pi, alpha=0.05, n) {
    
    w = 0:n
  
    pi.hat = w/n
    pmf = dbinom(x=w, size=n, prob=pi)

    p.tilde <- (w + qnorm(p = 1-alpha/2)^2 / 2) / (n + qnorm(p = 1-alpha/2)^2)
    
    wilson.CI_lower.bound = round(p.tilde - qnorm(p = 1-alpha/2) * sqrt(n) / (n + qnorm(p = 1-alpha/2)^2) * sqrt(pi.hat*(1-pi.hat) + qnorm(p =
1-alpha/2)^2/(4*n)), 4)
    
    wilson.CI_upper.bound = round(p.tilde + qnorm(p = 1-alpha/2) * sqrt(n) / (n + qnorm(p = 1-alpha/2)^2) * sqrt(pi.hat*(1-pi.hat) + qnorm(p =
1-alpha/2)^2/(4*n)), 4)

    # + ((qnorm(p = 1-alpha/2))^2/4*n)
    covered.pi = ifelse(test = pi>wilson.CI_lower.bound, yes = ifelse(test = pi<wilson.CI_upper.bound, yes=1, no=0), no=0)
    
    wilson.CI.true.coverage = sum(covered.pi*pmf)
    
    wilson.df = data.frame(w, pi.hat, round(data.frame(pmf, wilson.CI_lower.bound,wilson.CI_upper.bound),4), covered.pi)
    
    return(wilson.df)
  }
  
  wilson.df = wilson.CI.true.coverage(pi=0.6, alpha=0.05, n=n)
  wilson.CI.true.coverage.level = sum(wilson.df$covered.pi*wilson.df$pmf)
  
  # Let's compute the ture coverage for a sequence of pi
  pi.seq = seq(0.01,0.99, by=0.01)
  wilson.CI.true.matrix = matrix(data=NA,nrow=length(pi.seq),ncol=2)
  counter=1
  for (pi in pi.seq) {
      wilson.df2 = wilson.CI.true.coverage(pi=pi, alpha=0.05, n=n)
      #print(paste('True Coverage is', sum(wald.df2$covered.pi*wald.df2$pmf)))
      wilson.CI.true.matrix[counter,] = c(pi,sum(wilson.df2$covered.pi*wilson.df2$pmf))
      counter = counter+1
  }
  str(wilson.CI.true.matrix)
  wilson.CI.true.matrix[1:5,]
  
  # Plot the true coverage level (for given n and alpha)
  plot(x=wilson.CI.true.matrix[,1],
       y=wilson.CI.true.matrix[,2],
       ylim=c(0,1),
       main = "Wilson C.I. True Confidence Level Coverage", xlab=expression(pi),
       ylab="True Confidence Level",
       type="l")
  abline(h=1-alpha, lty="dotted")
}

two.two(10)
two.two(50)
two.two(100)
two.two(500)


```



\newpage
# 2: Binary Logistic Regression (2 points)
**Do Exercise 8 a, b, c, and d on page 131 of Bilder and Loughin's textbook**. 
Please write down each of the questions. The dataset for this question is stored in the file *"placekick.BW.csv"* which is provided to you. 

In general, all the R codes and datasets used in Bilder and Loughin's book are provided on the book's website: [chrisbilder.com](http://www.chrisbilder.com/categorical/index.html)

For **question 8b**, in addition to answering the question, re-estimate the model in part (a) using $"Sun"$ as the base level category for $Weather$.

```{r, message=FALSE}
library(car)
placekick <- read.csv("placekick.BW.csv", header=TRUE, sep=",")
head(placekick)
```

Continuing Exercise 7, use the Distance, Weather, Wind15, Temperature, Grass, Pressure, and Ice explanatory variables as linear terms in a new logistic regression model and complete the following:

(a) Estimate the model and properly define the indicator variables used within it.

```{r, message=FALSE}

model.2.a <- glm(Good ~ Distance + Weather + Wind15 + Temperature + Grass + Pressure + Ice, family = binomial(link = logit), data = placekick)
summary(model.2.a)
model.2.a$coefficients

```
**The indicator variables are Weather(4 levels) and Temperature (3 levels). Other onces include Wind15, Grass, Pressure, and Ice (2 levels).**

**The model equation is: Y = 5.74018455(Intercept) - 0.10959961(Distance) - 0.08302951(WeatherInside) - 0.44419298(WeatherSnowRain) - 0.24758206(WeatherSun) - 0.24377683(Wind15) + 0.25001316(TemperatureHot) + 0.23493183(TemperatureNice) - 0.32843455(Grass) + 0.27017353(PressureY) - 0.87613251(Ice).**

(b) The authors use "Sun" as the base level category for Weather, which is not the default level that R uses. Describe how "Sun" can be specified as the base level in R.

```{r, message=FALSE}
placekick$Weather <- relevel(placekick$Weather, ref = "Sun")

model.2.b <- glm(Good ~ Distance + Weather + Wind15 + Temperature + Grass + Pressure + Ice, family = binomial(link = logit), data = placekick)
summary(model.2.b)
model.2.b$coefficients


```
**In order to change the base categoery, we use relevel, and specificy what we want it to be. In the case above, we are using sun.**

**The model equation is: Y = 5.4926025(Intercept) - 0.1095996(Distance) - 0.2475821(WeatherClouds) - 0.1645526(WeatherInside) - 0.1966109(WeatherSnowRain) - 0.2437768(Wind15) + 0.2500132(TemperatureHot) + 0.2349318(TemperatureNice) - 0.3284346(Grass) + 0.2701735(PressureY) - 0.8761325(Ice).**

(c) Perform LRTs for all explanatory variables to evaluate their importance within the model. Discuss the results.

```{r, message=FALSE}
Anova(model.2.a, test = "LR")
```
**Distance is the most significant, with a p value of 2e-16. The presence of grass is important as well, with a p balue of 0.037, which is significant at the 5% level. Ice is significant at the 10% level.**

(d) Estimate an appropriate odds ratio for distance, and compute the corresponding confidence interval. Interpret the odds ratio.

```{r, message=FALSE}
print("Odds Ratio")
exp(model.2.a$coefficients[2])
print("Confidence Interval")
beta.ci <- confint(object = model.2.a, parm = "Distance", level = 0.95)
as.numeric(exp(beta.ci))

print("----------------------")
# Alternatively, we can do it only on the significant variables. For this, we get:
model.2.d <- model.2.b <- glm(Good ~ Distance + Grass + Ice, family = binomial(link = logit), data = placekick)
print("Odds Ratio")
exp(model.2.d$coefficients[2])
print("Confidence Interval")
beta.ci <- confint(object = model.2.d, parm = "Distance", level = 0.95)
as.numeric(exp(beta.ci))

```
**Interpretation - odds of success changes by 0.896 times for every 1 year decrease in distance of the kick.**

\newpage
# 3: Binary Logistic Regression (2 points)
The dataset *"admissions.csv"* contains a small sample of graduate school admission data from a university. The variables are specificed below:

  1. admit - the depenent variable that takes two values: $0,1$ where $1$ denotes *admitted* and $0$ denotes *not admitted*
  
  2. gre - GRE score
  
  3. gpa - College GPA
  
  4. rank - rank in college major

Suppose you are hired by the University's Admission Committee and are charged to analyze this data to quantify the effect of GRE, GPA, and college rank on admission probability. We will conduct this analysis by answering the follwing questions:

**Question 3.1:** Examine the data and conduct EDA

```{r, message=FALSE}
library(psych)

admissions <- read.csv("admissions.csv", header=TRUE, sep=",")
admissions$X <- NULL

# Basic EDA
print("DESCRIBE")
describe(admissions)
print("SUMMARY")
summary(admissions)

hist(admissions$gpa, breaks = 40, col="blue")

cdplot(factor(admissions$admit) ~ admissions$gpa)
cdplot(factor(admissions$admit) ~ admissions$gre)
cdplot(factor(admissions$admit) ~ admissions$rank)

plot(density(admissions$rank))



```
**Biggest point is that there is no missing data. The means/SD can be seen in describe. It seems as if rank is the biggest predictor of admission, with a significant cliff at every rank level.**


**Question 3.2:** Estimate a binary logistic regression using the following set of explanatory variables: $gre$, $gpa$, $rank$, $gre^2$, $gpa^2$, and $gre \times gpa$, where $gre \times gpa$ denotes the interaction between $gre$ and $gpa$ variables

**Here, I make rank a factor. This is because it is not a continous variable.**
```{r, message=FALSE}

model.3.2 <- glm(formula = admit ~ gre + gpa + factor(rank) + I(gre^2) + I(gpa^2) + gpa:gre, family = binomial(link = logit), data = admissions)
summary(model.3.2)
model.3.2$coefficients

```
**The equation is: Y = -7.325485e+00(Intercept) + 1.860245e-02(gre) - 1.777052e-01(gpa) - 7.130421e-01(rank2) - 1.341372e+00(rank3) - 1.595493e+00(rank4) + 3.070427e-06(gre^2) + 6.698514e-01(gpa^2) - 5.887872e-03(gre:gpa).**

**Question 3.3:** Test the hypothesis that GRE has no effect on admission using the likelihood ratio test

```{r, message=FALSE}
library(car)

Anova(model.3.2, test = "LR")

model.3.3 <- glm(formula = admit ~ gpa + factor(rank)  + I(gpa^2), family = binomial(link = logit), data = admissions)
anova(model.3.2, model.3.3, test = "Chisq")
```
**Based on these two models, we can see that  GRE does have an effect on admission. Using the Anova test, the interaction of gre:gpa have an affect at the 10% level. Using the anova test to further understand the difference, we see signifiance at the 5% level.**

**Question 3.4:** What is the estimated effect of college GPA on admission?

```{r, message=FALSE}
exp(model.3.2$coefficients)
# Looking at the result based on gre = 700, rank = 2
effect <- function(gpa) {
  predict.data <- data.frame(gpa =gpa, gre = 700, rank = 2)
  linear.pred <- predict(object = model.3.2, newdata = predict.data, type = "link", se = TRUE)
  pi.hat <- exp(linear.pred$fit) / (1 + exp(linear.pred$fit))
  return (pi.hat)
}
effect(2.0)
effect(2.3)
effect(2.7)
effect(3.0)
effect(3.3)
effect(3.7)
effect(4.0)

c <- .3

# Using log odds here
admission <- exp(c*model.3.2$coefficients['gpa'] + c * 6 * model.3.2$coefficients['I(gpa^2)'])

admission
```
**We can see a 0.3 increase in GPA increases the odds of admission by 3.16x**

**Question 3.5:** Construct the confidence interval for the admission probability for the students with $GPA = 3.3$, $GRE = 720$, and $rank=1$

```{r, message=FALSE}


alpha = 0.05
predict.data <- data.frame(gpa = (3.3), gre = 720, rank = 1)
predict(object = model.3.2, newdata = predict.data, type = "response")
linear.pred <- predict(object = model.3.2, newdata = predict.data, type = "link", se = TRUE)
linear.pred$fit
pi.hat <- exp(linear.pred$fit) / (1 + exp(linear.pred$fit))

CI.lin.pred <- linear.pred$fit + qnorm(p = c(alpha/2, 1-alpha/2)) * linear.pred$se

CI.pi <- exp(CI.lin.pred)/(1+exp(CI.lin.pred))

data.frame(predict.data, pi.hat, lower = CI.pi[1], upper= CI.pi[2])


```
**The expected probability of admission is 0.5692897 with an interval of [0.4366982, 0.6926379]**

\newpage
# 4. Binary Logistic Regression (2 points)

Load the `Mroz` data set that comes with the *car* library (this data set is used in the week 2 live session file).

```{r, message=FALSE}
library(car)
head(Mroz)
```

**Question 4.1:** Estimate a linear probability model using the same specification as in the binary logistic regression model estimated in the week 2 live session. Interpret the model results. Conduct model diagnostics. Test the CLM model assumptions.

```{r, message=FALSE}
library(car)
# REMEMBER HERE YOU ARE CHANGING FROM FACTOR TO NUMBER
model.4.1 <- lm(formula = as.numeric(lfp) ~ k5 + k618 + age + wc + hc + lwg + inc, data = Mroz)
summary(model.4.1)$coefficients
# summary(model.4.1)
plot(model.4.1)

# Binomial Mass Function
```
**The equation of the model is: 1.143547836(Intercept) - 0.294835968(k5) - 0.011215027(k618) - 0.012741098(age) + 0.163679033(wcyes) + 0.018951039(hcyes) + 0.122740218(lwg) - 0.006760342(inc).**
**Starting with the residuals, we can see there is a clear pattern in them (based of the 0 residusal line). This breaks the hetroskedastic assumption of linear regression. Next, the QQ plot should not be S shaped, and should be more of a straight line. For the scale v location, you want to see a horizational line with points that are equally spaced. This is clealy not the case for the hyperbolic results we seeFor the lev v residual plot, we can see there are a decent amount of influential cases which can be concerning. Thus, all 4 graphs show how linear regression is a terrible way to model the Mroz dataset.**

**Question 4.2:** Estimate a binary logistic regression with `lfp`, which is a binary variable recoding the participation of the females in the sample, as the dependent variable. The set of explanatory variables includes `age`, `inc`, `wc`, `hc`, `lwg`, `totalKids`, and a quadratic term of `age`, called `age_squared`, where `totalKids` is the total number of children up to age $18$ and is equal to the sum of `k5` and `k618`.

```{r, message=FALSE}
totalKids <- Mroz$k5 + Mroz$k618
age_squared <- I(Mroz$age^2)

model.4.2 <- glm(formula = lfp ~ age + inc + hc + wc + lwg + age_squared + totalKids, family = binomial(link = logit), data = Mroz)
summary(model.4.2)
model.4.2$coefficients

```
**log likelihood =  -5.150511297 + 0.311895142(age) - 0.033434758(inc) +0.713378272(wcyes) + 0.550747255(lwg) - 0.004051356(age_squared) - 0.221626269(totalKids)**

**Question 4.3:** Is the age effect statistically significant? 

```{r, message=FALSE}
Anova(model.4.2, test = "LR")
```
**The age effect, as well as age squared, is significant at the .001% level.**

**Question 4.4:** What is the effect of a decrease in age by $5$ years on the odds of labor force participation for a female who was $45$ years of age.

```{r, message=FALSE}
c = -5
Age = 45

OR.change <- exp(c*(coef(model.4.2)[['age']] + coef(model.4.2)[['age_squared']]*(2*Age + c)))
OR.change
```
**The odds of labor participation go up by 1.17x**

**Question 4.5:** Estimate the profile likelihood confidence interval of the probability of labor force participation for females who were $40$ years old, had income equal to $20$, did not attend college, had log wage equal to 1, and did not have children.

```{r, message=FALSE}
alpha = 0.05

model.4.5 <- glm(formula = lfp ~ age + inc + wc + lwg + age_squared + totalKids, family = binomial(link = logit), data = Mroz)
predict.data <- data.frame(age = 40, inc = 20, wc = "no", lwg = 1, age_squared = 1600, totalKids = 0)
predict(object = model.4.5, newdata = predict.data, type =
"response")

linear.pred <- predict(object = model.4.5, newdata = predict.data, type = "link", se = TRUE)
pi.hat <- exp(linear.pred$fit) / (1 + exp(linear.pred$fit))

CI.lin.pred <- linear.pred$fit + qnorm(p = c(alpha/2, 1-alpha/2)) * linear.pred$se

CI.pi <- exp(CI.lin.pred)/(1+exp(CI.lin.pred))

data.frame(predict.data, pi.hat, lower = CI.pi[1], upper= CI.pi[2])


```
**The p hat is 0.673746, with a CI of [0.5942122, 0.7443968]**

\newpage
# 5: Maximum Likelihood (2 points)

**Question 18 a and b of Chapter 3 (page 192,193)**

For the wheat kernel data (*wheat.csv*), consider a model to estimate the kernel condition using the density explanatory variable as a linear term.

```{r, message=FALSE}
library(nnet)

wheat <- read.csv('wheat.csv')
head(wheat)
levels(wheat$type)
wheat.model <- multinom(formula = type ~ density, data = wheat)

```
**We can see for log (scab/healthy), the equation is 29.37827 - 24.56215(Density). For log (Sprout/healthy), it is  19.12165 - 15.47633. When I say scab, I mean the predicted prob of success of scab.**

**Question 5.1** Write an R function that computes the log-likelihood
function for the multinomial regression model. Evaluate the function at the parameter estimates produced by multinom(), and verify that your computed value is the same as that produced by logLik() (use the object saved from multinom() within this function).

```{r, message=FALSE}

# Calculate log_likelihood by hand
logL <- function(beta, x, Y) {
  
  beta.1.3 <- exp(beta[1] + beta[3] * x)
  beta.2.4 <- exp(beta[2] + beta[4] * x)             

  pi.h <- 1/(1 + beta.1.3 + beta.2.4)
  Y.h <- ifelse(Y == "Healthy", 1, 0)

  pi.sc <- beta.1.3/(1 + beta.1.3 + beta.2.4)
  Y.sc <- ifelse(Y == "Scab", 1, 0)

  pi.sp <- beta.2.4/(1 + beta.2.4 + beta.1.3)
  Y.sp <- ifelse(Y == "Sprout", 1, 0)
  

  sum(Y.h *log(pi.h) + Y.sc*log(pi.sc) + Y.sp * log(pi.sp))

}

logL(beta = summary(wheat.model)$coefficients, x = wheat$density, Y = wheat$type)
logLik(wheat.model)
```
**Confirmed, the two values are the same (-229.7123).**

**Question 5.2** Maximize the log-likelihood function using optim() to obtain the MLEs and the estimated covariance matrix. Compare your answers to what is obtained by multinom(). Note that to obtain starting values for optim(), one approach is to estimate separate logistic regression models for $log \left( \frac{\pi_2}{\pi_1} \right)$ and $log \left( \frac{\pi_3}{\pi_1} \right)$. These models are estimated only for those observations that have the corresponding responses (e.g., a $Y = 1$ or $Y = 2$ for $log \left( \frac{\pi_2}{\pi_1} \right)$).

```{r, message=FALSE}
# Using page 72 as reference from the book
mod.fit.optim <- optim(summary(wheat.model)$coefficients, logL, x = wheat$density, Y = wheat$type, hessian = TRUE, control=list(fnscale =-1), method ="BFGS")
mod.fit.optim$value
mod.fit.optim$hessian
```
**The optimized log likelihood and Hessian matrix can be seen above with the use of the optim function.**

